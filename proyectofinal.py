# -*- coding: utf-8 -*-
"""ProyectoFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fgRmrRC6958n3y97BOtg50m6PJljhPPT

Importar librerias
"""

import numpy as np 
import pandas as pd
import re
import string
import os
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

import warnings
warnings.filterwarnings('ignore')

import tensorflow as tf

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, Flatten
from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers

"""Leer datos"""

df_train = pd.read_csv("labeledTrainData.tsv.zip", header=0, delimiter="\t", quoting=3)
df_test = pd.read_csv("testData.tsv.zip", header=0, delimiter="\t", quoting=3)
df_train["review"][5]

"""Limpieza de datos"""

def data_cleaning(raw_data):
    raw_data = raw_data.translate(str.maketrans('', '', string.punctuation + string.digits))
    words = raw_data.lower().split()
    stops = set(stopwords.words("english"))
    useful_words = [w for w in words if not w in stops]
    return( " ".join(useful_words))

df_train['review']=df_train['review'].apply(data_cleaning)
df_test["review"]=df_test["review"].apply(data_cleaning)
df_train["review"][5]

"""Tokenizar texto"""

y = df_train["sentiment"].values
train_reviews = df_train["review"]
test_reviews = df_test["review"]

max_features = 6000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(train_reviews))
list_tokenized_train = tokenizer.texts_to_sequences(train_reviews)
list_tokenized_test = tokenizer.texts_to_sequences(test_reviews)
list_tokenized_test[5]

"""Settear x"""

max_length = 370
X_train = pad_sequences(list_tokenized_train, maxlen=max_length)
X_test = pad_sequences(list_tokenized_test, maxlen=max_length)

"""Clase para guardar memoria"""

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epochs, logs={}):
        if logs.get('accuracy') > 0.95:
            print('\n Stopped Training!\n')
            self.model.stop_training = True

def train_model(model, model_name, n_epochs, batch_size, X_data, y_data, validation_split):    
    checkpoint_path = model_name+"_cp-{epoch:04d}.ckpt"
    checkpoint_dir = os.path.dirname(checkpoint_path)
    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)
    callbacks_earlystop = myCallback()
    history = model.fit(
        X_data,
        y_data,
        steps_per_epoch=batch_size,
        epochs=n_epochs,
        validation_split=validation_split,
        verbose=1,
        callbacks=[cp_callback, callbacks_earlystop]
    )
    return history

"""Crear clase del modelo"""

class Model():
  def __new__(self):
    embed_size = 128
    model = Sequential()
    model.add(Embedding(max_features, embed_size))
    model.add(Bidirectional(LSTM(75, return_sequences = True)))
    model.add(GlobalMaxPool1D())
    model.add(Dense(16, activation="relu"))
    model.add(Dropout(0.03))
    model.add(Dense(8, activation="relu"))
    model.add(Dropout(0.1))
    model.add(Dense(1, activation="sigmoid"))

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return model
    
model = Model()
print(model.summary())

"""Entrenar modelo"""

trained_model = train_model(model, "model", 10, 64, X_train, y, 0.2)

"""Obtener f1-score y matriz de confusion"""

pred = model.predict(X_test)
y_pred = (pred > 0.5)
df_test["sentiment"] = df_test["id"].map(lambda x: 1 if int(x.strip('"').split("_")[1]) >= 5 else 0)
y_test = df_test["sentiment"]
cf_matrix = confusion_matrix(y_pred, y_test)
f1_score_calc = cf_matrix[0][0] / (cf_matrix[0][0] + 0.5 * (cf_matrix[0][1] + cf_matrix[1][0]))

print('F1-score: %.3f' % f1_score_calc)
print("Confusion Matrix : ", cf_matrix)

model.save("model.h5")

# """--------------------------------------------------------------------------------

# Pruebas manuales
# """

# df_my_test = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Datasets/my_test.csv", header=0, delimiter="\t", quoting=3)
# x_my_test = df_my_test["review"]
# x_my_test

# """Tokenizar"""

# max_features = 6000
# tokenizer = Tokenizer(num_words=max_features)
# tokenizer.fit_on_texts(list(train_reviews))
# tokenized_my_test = tokenizer.texts_to_sequences(x_my_test)
# tokenized_my_test

# x_to_predict = pad_sequences(tokenized_my_test, maxlen=max_length)

# pred_my_test = model.predict(x_to_predict)
# y_pred_my_test = (pred_my_test > 0.5)
# y_pred_my_test

# result=zip(x_my_test, y_pred_my_test)
# for i in result:
#  print(i)